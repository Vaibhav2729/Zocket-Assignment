{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d35afd",
   "metadata": {
    "papermill": {
     "duration": 0.023056,
     "end_time": "2024-04-16T04:14:14.623824",
     "exception": false,
     "start_time": "2024-04-16T04:14:14.600768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=''># LLM Prompt Recovery using LLM\n",
    "This notebook investigates the potential of pre-trained or fine-tuned LLMs to recover the original prompt used to generate a text. \n",
    "\n",
    "### Table of contentsÂ¶\n",
    "* [Offline Installation](#installation)\n",
    "* [Importing Libraries](#libraries)\n",
    "* [Model Training](#training)\n",
    "    - [Fine-tuning Gemma-2b model using Keras library](#gemma-2b)\n",
    "    - [Fine-tuning Gemma-7b model using pytorch library](#pretrained)\n",
    "* [Model Inference](#infer)\n",
    "    - [Generate prompts using fine-tuned Phi LLM](#phi)\n",
    "    - [Generate the prompts using pretrained Gemma-7b LLM](#llm)\n",
    "    - <a href='#mistral'>Generate the prompts using pretrained Mistral-7b LLM (version 2)]</a>\n",
    "    - [Modify the prompts using Spacy](#mean-prompt)\n",
    "    - [Evaluation](#evaluation)\n",
    "    \n",
    "### Change Logs:\n",
    "\n",
    "- [version 7] (LB=0.63) use Spacy library to rewrite the prompts with most representative words  \n",
    "- [version 5] (LB=0.63) use pretrained Gemma-7b and only one rewrite template\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5eafe4",
   "metadata": {
    "papermill": {
     "duration": 0.023446,
     "end_time": "2024-04-16T04:14:14.670209",
     "exception": false,
     "start_time": "2024-04-16T04:14:14.646763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Offline Installation <a class=\"anchor\"  id=\"installation\"></a>\n",
    "This section details the installation of required packages for offline use: bitandbytes (if applicable), accelerate, datasets, transformers, TRL (supervised fine-tuning trainer), and PEFT (LoRa layers), optimum (transformer performance optimization tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7610c364",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-04-16T04:14:14.715863Z",
     "iopub.status.busy": "2024-04-16T04:14:14.715500Z",
     "iopub.status.idle": "2024-04-16T04:16:13.382376Z",
     "shell.execute_reply": "2024-04-16T04:16:13.381292Z"
    },
    "papermill": {
     "duration": 118.692573,
     "end_time": "2024-04-16T04:16:13.384873",
     "exception": false,
     "start_time": "2024-04-16T04:14:14.692300",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/bitsandbytes\r\n",
      "Processing /kaggle/input/bitsandbytes/bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (from -r /kaggle/input/bitsandbytes/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes->-r /kaggle/input/bitsandbytes/requirements.txt (line 1)) (1.3.0)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.43.0\r\n",
      "Looking in links: /kaggle/input/accelerate\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/accelerate/requirements.txt (line 1)) (0.28.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r /kaggle/input/accelerate/requirements.txt (line 1)) (1.3.0)\r\n",
      "Looking in links: /kaggle/input/transformers\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/transformers/requirements.txt (line 1)) (4.39.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/transformers/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Looking in links: /kaggle/input/datasets-installation\r\n",
      "Processing /kaggle/input/datasets-installation/datasets-2.16.0-py3-none-any.whl (from -r /kaggle/input/datasets-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (15.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.6)\r\n",
      "Processing /kaggle/input/datasets-installation/dill-0.3.7-py3-none-any.whl (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.1.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.70.16)\r\n",
      "Processing /kaggle/input/datasets-installation/fsspec-2023.10.0-py3-none-any.whl (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2024.2.2)\r\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Processing /kaggle/input/datasets-installation/multiprocess-0.70.15-py310-none-any.whl (from datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0->-r /kaggle/input/datasets-installation/requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: fsspec, dill, multiprocess, datasets\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2024.2.0\r\n",
      "    Uninstalling fsspec-2024.2.0:\r\n",
      "      Successfully uninstalled fsspec-2024.2.0\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.8\r\n",
      "    Uninstalling dill-0.3.8:\r\n",
      "      Successfully uninstalled dill-0.3.8\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.16\r\n",
      "    Uninstalling multiprocess-0.70.16:\r\n",
      "      Successfully uninstalled multiprocess-0.70.16\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.18.0\r\n",
      "    Uninstalling datasets-2.18.0:\r\n",
      "      Successfully uninstalled datasets-2.18.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "gcsfs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\r\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\r\n",
      "s3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed datasets-2.16.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15\r\n",
      "Looking in links: /kaggle/input/transformer-reinforcement-learning\r\n",
      "Processing /kaggle/input/transformer-reinforcement-learning/trl-0.8.1-py3-none-any.whl (from -r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.39.3)\r\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.28.0)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.16.0)\r\n",
      "Processing /kaggle/input/transformer-reinforcement-learning/tyro-0.7.3-py3-none-any.whl (from trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.10.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.15)\r\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (13.7.0)\r\n",
      "Processing /kaggle/input/transformer-reinforcement-learning/shtab-1.7.1-py3-none-any.whl (from tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (5.9.3)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (15.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.1.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.70.15)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.9.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.17.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (2023.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl->-r /kaggle/input/transformer-reinforcement-learning/requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: shtab, tyro, trl\r\n",
      "Successfully installed shtab-1.7.1 trl-0.8.1 tyro-0.7.3\r\n",
      "Looking in links: /kaggle/input/peft-installation\r\n",
      "Processing /kaggle/input/peft-installation/peft-0.9.0-py3-none-any.whl (from -r /kaggle/input/peft-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.28.0)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.10.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (0.15.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r /kaggle/input/peft-installation/requirements.txt (line 1)) (1.3.0)\r\n",
      "Installing collected packages: peft\r\n",
      "Successfully installed peft-0.9.0\r\n",
      "Looking in links: /kaggle/input/optimum-installation\r\n",
      "Processing /kaggle/input/optimum-installation/optimum-1.18.0-py3-none-any.whl (from -r /kaggle/input/optimum-installation/requirements.txt (line 1))\r\n",
      "Processing /kaggle/input/optimum-installation/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: transformers<4.40.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.39.3)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.16.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.10.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.40.0,>=4.26.0->transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.40.0,>=4.26.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.20.3)\r\n",
      "Processing /kaggle/input/optimum-installation/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (15.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.1.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (0.70.15)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.9.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum->-r /kaggle/input/optimum-installation/requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: humanfriendly, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.18.0\r\n",
      "Looking in links: /kaggle/input/sentence-transformers\r\n",
      "Processing /kaggle/input/sentence-transformers/sentence_transformers-2.6.1-py3-none-any.whl (from -r /kaggle/input/sentence-transformers/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (4.39.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (4.66.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (0.22.2)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2023.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (0.4.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers->-r /kaggle/input/sentence-transformers/requirements.txt (line 1)) (1.3.0)\r\n",
      "Installing collected packages: sentence_transformers\r\n",
      "Successfully installed sentence_transformers-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "# Install bitsandbytes for loading the LLM model faster\n",
    "!pip install --no-index --find-links=/kaggle/input/bitsandbytes -r /kaggle/input/bitsandbytes/requirements.txt\n",
    "!pip install --no-index --find-links=/kaggle/input/accelerate -r /kaggle/input/accelerate/requirements.txt\n",
    "!pip install --no-index --find-links=/kaggle/input/transformers -r /kaggle/input/transformers/requirements.txt\n",
    "# Install datasets\n",
    "!pip install --no-index --find-links=/kaggle/input/datasets-installation -r /kaggle/input/datasets-installation/requirements.txt\n",
    "# Install TRL for using Supervised Fine-tuning Trainer\n",
    "!pip install --no-index --find-links=/kaggle/input/transformer-reinforcement-learning -r /kaggle/input/transformer-reinforcement-learning/requirements.txt\n",
    "# Install PEFT\n",
    "!pip install --no-index --find-links=/kaggle/input/peft-installation -r /kaggle/input/peft-installation/requirements.txt\n",
    "# Install optimum\n",
    "!pip install --no-index --find-links=/kaggle/input/optimum-installation -r /kaggle/input/optimum-installation/requirements.txt\n",
    "# Install sentence transformer\n",
    "!pip install --no-index --find-links=/kaggle/input/sentence-transformers -r /kaggle/input/sentence-transformers/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a3d23",
   "metadata": {
    "papermill": {
     "duration": 0.031637,
     "end_time": "2024-04-16T04:16:13.448704",
     "exception": false,
     "start_time": "2024-04-16T04:16:13.417067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libraries <a class=\"anchor\"  id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273f0774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:13.512552Z",
     "iopub.status.busy": "2024-04-16T04:16:13.512182Z",
     "iopub.status.idle": "2024-04-16T04:16:33.622354Z",
     "shell.execute_reply": "2024-04-16T04:16:33.621502Z"
    },
    "papermill": {
     "duration": 20.145567,
     "end_time": "2024-04-16T04:16:33.624722",
     "exception": false,
     "start_time": "2024-04-16T04:16:13.479155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 04:16:22.893135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 04:16:22.893247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 04:16:23.030910: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from string import Template\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "# Transformer\n",
    "from accelerate import Accelerator\n",
    "import transformers\n",
    "from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig, AutoConfig, TrainingArguments)\n",
    "# Supervised Trainser\n",
    "from datasets import Dataset\n",
    "\n",
    "# Split data into training and test (valid) dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For quantization\n",
    "import bitsandbytes, accelerate\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d23884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:33.690156Z",
     "iopub.status.busy": "2024-04-16T04:16:33.688870Z",
     "iopub.status.idle": "2024-04-16T04:16:33.697088Z",
     "shell.execute_reply": "2024-04-16T04:16:33.696075Z"
    },
    "papermill": {
     "duration": 0.042829,
     "end_time": "2024-04-16T04:16:33.699213",
     "exception": false,
     "start_time": "2024-04-16T04:16:33.656384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ctypes, gc\n",
    "import torch\n",
    "\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "# Set the GPUs\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec617e08",
   "metadata": {
    "papermill": {
     "duration": 0.034411,
     "end_time": "2024-04-16T04:16:33.765174",
     "exception": false,
     "start_time": "2024-04-16T04:16:33.730763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training <a class='anchor' id='training'></a>\n",
    "## Fine-tuning Gemma-2b model using Keras library <a class=\"anchor\"  id=\"gemma-2b\"></a>\n",
    "\n",
    "\n",
    "Ref: @JUAN MERINO [Fine Tuning with Gemma 2b](https://www.kaggle.com/code/juanmerinobermejo/fine-tuning-with-gemma-2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0047ad94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:33.827734Z",
     "iopub.status.busy": "2024-04-16T04:16:33.826744Z",
     "iopub.status.idle": "2024-04-16T04:16:35.226415Z",
     "shell.execute_reply": "2024-04-16T04:16:35.225555Z"
    },
    "papermill": {
     "duration": 1.432662,
     "end_time": "2024-04-16T04:16:35.228817",
     "exception": false,
     "start_time": "2024-04-16T04:16:33.796155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import keras and Keras-NLP for training\n",
    "import keras\n",
    "import keras_nlp\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bab856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.292357Z",
     "iopub.status.busy": "2024-04-16T04:16:35.291967Z",
     "iopub.status.idle": "2024-04-16T04:16:35.298799Z",
     "shell.execute_reply": "2024-04-16T04:16:35.297492Z"
    },
    "papermill": {
     "duration": 0.041383,
     "end_time": "2024-04-16T04:16:35.301106",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.259723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma_2b_en_adapter\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    model_name = 'gemma_2b_en'\n",
    "    model_path = '/kaggle/input/gemma/keras/gemma_2b_en/2'\n",
    "    data_path = '/kaggle/input/rewritten-texts-with-gemma-2b/rewritten_texts_csv.csv'\n",
    "    output_path = f'outputs'\n",
    "    model_save_path =  f'{model_name}_adapter'\n",
    "    \n",
    "    # Model training argument\n",
    "    epochs=20\n",
    "    batch_size=1 \n",
    "    max_length=512 \n",
    "    lr = 1e-3\n",
    "    \n",
    "print(CFG.model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24fbe1",
   "metadata": {
    "papermill": {
     "duration": 0.033647,
     "end_time": "2024-04-16T04:16:35.366744",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.333097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load and Train the model\n",
    "\n",
    "Quantization technique is used to reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc01ee4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.482890Z",
     "iopub.status.busy": "2024-04-16T04:16:35.482477Z",
     "iopub.status.idle": "2024-04-16T04:16:35.489955Z",
     "shell.execute_reply": "2024-04-16T04:16:35.488949Z"
    },
    "papermill": {
     "duration": 0.042726,
     "end_time": "2024-04-16T04:16:35.492262",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.449536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load data and split into training and valid dataset\n",
    "def load_data():\n",
    "    df = pd.read_csv(CFG.data_path, encoding='latin-1')\n",
    "    output_texts = []\n",
    "    for index in range(len(df)):\n",
    "        row = df.iloc[index]\n",
    "        original_text = row['original_text']\n",
    "        prompt = row['prompt']\n",
    "        rewritten_text = row['rewritten_text']\n",
    "        # Format the prompt with original and rewritten texts\n",
    "        formatted_prompt = f\"\"\"Original Text:\\n{original_text}\\n\\n\n",
    "                               Prompt:\\n{prompt}\\n\\n\n",
    "                               Rewritten text:\\n{rewritten_text}\"\"\"\n",
    "        if len(formatted_prompt) < CFG.max_length:\n",
    "            output_texts.append(formatted_prompt)\n",
    "    del df\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b6076c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.560432Z",
     "iopub.status.busy": "2024-04-16T04:16:35.559497Z",
     "iopub.status.idle": "2024-04-16T04:16:35.570383Z",
     "shell.execute_reply": "2024-04-16T04:16:35.569271Z"
    },
    "papermill": {
     "duration": 0.047432,
     "end_time": "2024-04-16T04:16:35.572628",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.525196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Load the training data\n",
    "    training_data = load_data() \n",
    "    # Load the Gemma and add lora layer\n",
    "    # ref: https://ai.google.dev/gemma/docs/lora_tuning\n",
    "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(CFG.model_name)\n",
    "    gemma_lm.summary()\n",
    "    # This will freeze all weights on the backbone,\n",
    "    # while enabling Lora on the query & value layers of the attention layers.\n",
    "    gemma_lm.backbone.enable_lora(rank=4)\n",
    "    gemma_lm.preprocessor.sequence_length = CFG.max_length\n",
    "    # Create the optimizer (AdamW)\n",
    "    optimizer = keras.optimizers.AdamW(learning_rate=CFG.lr,\n",
    "                                       weight_decay=0.001,\n",
    "                                       beta_1=0.9,\n",
    "                                       beta_2=0.999)\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "    # Add optimizer, loss function and evalution metrics\n",
    "    gemma_lm.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     optimizer=optimizer,\n",
    "                     weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "    # Train the model with \n",
    "    gemma_lm.fit(training_data, epochs=CFG.epochs, batch_size=1, verbose=1)      \n",
    "    # Save the model\n",
    "    gemma_lm.save_weights(CFG.model_save_path)\n",
    "    gemma_lm.preprocessor.tokenizer.save_assets(CFG.model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a988e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.640002Z",
     "iopub.status.busy": "2024-04-16T04:16:35.639218Z",
     "iopub.status.idle": "2024-04-16T04:16:35.644349Z",
     "shell.execute_reply": "2024-04-16T04:16:35.643295Z"
    },
    "papermill": {
     "duration": 0.041526,
     "end_time": "2024-04-16T04:16:35.646620",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.605094",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = False # True: Enable training, False: Infer only\n",
    "if TRAINING:\n",
    "    train_model()\n",
    "    os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1465a",
   "metadata": {
    "papermill": {
     "duration": 0.032799,
     "end_time": "2024-04-16T04:16:35.711886",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.679087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Fine-tuning Gemma-7b using pytorch library<a class=\"anchor\" id=\"pretrained\"></a>\n",
    "Fine-tuned pretrained LLM (Gemma/Mistral/Phi) to infer the testing data's prompt.\n",
    "\n",
    "- @ZHANSAYA YUSSUPOVA [Gemma 7B with LoRa | Prompt Recovery](https://www.kaggle.com/code/yujansaya/gemma-7b-with-lora-prompt-recovery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ac5e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.781268Z",
     "iopub.status.busy": "2024-04-16T04:16:35.780857Z",
     "iopub.status.idle": "2024-04-16T04:16:35.786921Z",
     "shell.execute_reply": "2024-04-16T04:16:35.785846Z"
    },
    "papermill": {
     "duration": 0.042748,
     "end_time": "2024-04-16T04:16:35.789265",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.746517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = 'gemma_7b'\n",
    "    model_paths = {'gemma_7b': '/kaggle/input/gemma/transformers/7b-it/2'}\n",
    "    model_path = model_paths[model_name]\n",
    "    \n",
    "    # Model training argument\n",
    "    data_path = '/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv'\n",
    "    model_save_path =  f'{model_name}_adapter'\n",
    "    max_length=150 # truncate the text to the first 150 words to avoid OOM issues.\n",
    "    NROWS = 10 # Read 1000 texts from dataset\n",
    "    batch_size = 1\n",
    "    lr = 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10859cdd",
   "metadata": {
    "papermill": {
     "duration": 0.032513,
     "end_time": "2024-04-16T04:16:35.855214",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.822701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07abd78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:35.921193Z",
     "iopub.status.busy": "2024-04-16T04:16:35.920498Z",
     "iopub.status.idle": "2024-04-16T04:16:35.927538Z",
     "shell.execute_reply": "2024-04-16T04:16:35.926531Z"
    },
    "papermill": {
     "duration": 0.0434,
     "end_time": "2024-04-16T04:16:35.929730",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.886330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    accelerator = Accelerator()\n",
    "    # Use quantization technique to reduce the memory usage\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "                                CFG.model_path,\n",
    "                                device_map = \"auto\",\n",
    "                                trust_remote_code = True,\n",
    "                                quantization_config=quantization_config)\n",
    "    model = accelerator.prepare(model)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507626bf",
   "metadata": {
    "papermill": {
     "duration": 0.03111,
     "end_time": "2024-04-16T04:16:35.993939",
     "exception": false,
     "start_time": "2024-04-16T04:16:35.962829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model training with prompts generated by Gemma LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef10186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.059733Z",
     "iopub.status.busy": "2024-04-16T04:16:36.059335Z",
     "iopub.status.idle": "2024-04-16T04:16:36.071248Z",
     "shell.execute_reply": "2024-04-16T04:16:36.070271Z"
    },
    "papermill": {
     "duration": 0.047547,
     "end_time": "2024-04-16T04:16:36.073354",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.025807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Formate the row (example) data with an instruction\n",
    "def formatting_func(example):\n",
    "    prompt = f\"\"\"Original Essay:\\n{example['original_text'][0]}\\n\\n\n",
    "               Rewritten Essay:\\n{example['rewritten_text'][0]}\\n\\n\n",
    "               Instruction:\\n Given are 2 essays, the Rewritten essay was created from the Original essay using the google Gemma model.\n",
    "               You are trying to understand how the original essay was transformed into a new version. \n",
    "               Analyzing the changes in style, theme, etc., please come up with a prompt that must have been used to guide the transformation from the original to the rewritten essay.\n",
    "               Only give me the PROMPT. Start directly with the prompt, that's all I need.\n",
    "               Output should be only line ONLY.\\n\\n\n",
    "               Response: \\n{example['rewrite_prompt'][0]}\"\"\"\n",
    "    return [prompt]\n",
    "\n",
    "def train_model(model, tokenizer):\n",
    "    # Load the training data\n",
    "    df = pd.read_csv(CFG.data_path, nrows=CFG.NROWS)\n",
    "    # Create the dataset\n",
    "    training_ds = Dataset.from_pandas(df)\n",
    "    # Tokenizer \n",
    "    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"original_text\"]), batched=True)\n",
    "    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"rewritten_text\"]), batched=True)\n",
    "    training_ds = training_ds.map(lambda samples: tokenizer(samples[\"rewrite_prompt\"]), batched=True)    \n",
    "    # Add PEFT (lora) layer\n",
    "    lora_config = LoraConfig(r=32, # Rank\n",
    "                             lora_alpha=32,\n",
    "                             target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \n",
    "                                             \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                             lora_dropout=0.05,\n",
    "                             bias=\"none\",\n",
    "                             task_type=TaskType.CAUSAL_LM)\n",
    "    # Training arguments\n",
    "    args = TrainingArguments(\n",
    "            per_device_train_batch_size=CFG.batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=10,\n",
    "            learning_rate=CFG.lr,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "    # Create a trainer (supervised fine-tuned trainer)\n",
    "    trainer = SFTTrainer(model=model,\n",
    "                         train_dataset=training_ds,\n",
    "                         args=args,\n",
    "                         peft_config=lora_config,\n",
    "                         formatting_func=formatting_func)\n",
    "    trainer.train()\n",
    "    # Save the model\n",
    "    trainer.save_model(CFG.model_save_path)\n",
    "    tokenizer.save_pretrained(CFG.save_path)\n",
    "    print(f\"Save the model to {CFG.save_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce901a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.139172Z",
     "iopub.status.busy": "2024-04-16T04:16:36.138795Z",
     "iopub.status.idle": "2024-04-16T04:16:36.143893Z",
     "shell.execute_reply": "2024-04-16T04:16:36.142958Z"
    },
    "papermill": {
     "duration": 0.040573,
     "end_time": "2024-04-16T04:16:36.146063",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.105490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = False # True: Enable training, False: Infer only\n",
    "if TRAINING:\n",
    "    model, tokenizer = load_model()\n",
    "    train_model(model, tokenizer)\n",
    "    os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0712b10",
   "metadata": {
    "papermill": {
     "duration": 0.030802,
     "end_time": "2024-04-16T04:16:36.207028",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.176226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Inference <a class='anchor' id='infer'></a>\n",
    "- [Load testing data](#load_data)\n",
    "- [Generate prompts using fine-tuned Phi LLM](#phi)\n",
    "- [Generate the prompts using pretrained Gemma-7b LLM](#llm)\n",
    "- [Generate the prompts using pretrained Mistral-7b LLM (version 2)](#mistral)\n",
    "- [Modify the prompts using Spacy](#mean-prompt): This is inspired by [Spacy mean-prompt modification (just CPU!)](https://www.kaggle.com/code/richolson/spacy-mean-prompt-modification-just-cpu/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209a527c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.272398Z",
     "iopub.status.busy": "2024-04-16T04:16:36.271986Z",
     "iopub.status.idle": "2024-04-16T04:16:36.278011Z",
     "shell.execute_reply": "2024-04-16T04:16:36.276965Z"
    },
    "papermill": {
     "duration": 0.041296,
     "end_time": "2024-04-16T04:16:36.280142",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.238846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Get device (CPUs or GPUs)\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_paths = {'phi': '/kaggle/input/phi/transformers/2/1',\n",
    "                   'gemma-7b': '/kaggle/input/gemma/transformers/7b-it/2', \n",
    "                   'mistral-7b': '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1',\n",
    "                   'mistral-7b-v2': '/kaggle/input/mistral-7b-it-v02',\n",
    "                   'mistral-8x7b':'/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1'\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918321",
   "metadata": {
    "papermill": {
     "duration": 0.03201,
     "end_time": "2024-04-16T04:16:36.345429",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.313419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load testing data <a class='anchor' id='load_data'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb244911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.413030Z",
     "iopub.status.busy": "2024-04-16T04:16:36.412637Z",
     "iopub.status.idle": "2024-04-16T04:16:36.448570Z",
     "shell.execute_reply": "2024-04-16T04:16:36.447554Z"
    },
    "papermill": {
     "duration": 0.072277,
     "end_time": "2024-04-16T04:16:36.450966",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.378689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>The competition dataset comprises text passage...</td>\n",
       "      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_text  \\\n",
       "id                                                      \n",
       "-1  The competition dataset comprises text passage...   \n",
       "\n",
       "                                       rewritten_text rewrite_prompt  \n",
       "id                                                                    \n",
       "-1  Here is your shanty: (Verse 1) The text is rew...              -  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the testing data\n",
    "test_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv', index_col='id')\n",
    "test_df[\"rewrite_prompt\"] = \"-\" # Empty\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59ecf3",
   "metadata": {
    "papermill": {
     "duration": 0.032461,
     "end_time": "2024-04-16T04:16:36.516297",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.483836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate prompts using fine-tuned Phi LLM <a class='anchor' id='phi'></a>\n",
    "Use the Microsoft Phi LLM fined-tuned by @LUMOS [phi2-public-data-sft-adapter](https://www.kaggle.com/models/mozhiwenmzw/phi2-public-data-sft-adapter/frameworks/PyTorch/variations/public-data-sft/versions/1) to generate the prompts of testing data\n",
    "\n",
    "Credits:\n",
    "- @Lumos [[0.61+]LLMPR phi2 sft model training](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-training)\n",
    "- @Lumos [[0.61+]LLMPR phi2 sft model generate infer](https://www.kaggle.com/code/mozhiwenmzw/0-61-llmpr-phi2-sft-model-generate-infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65298ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.584119Z",
     "iopub.status.busy": "2024-04-16T04:16:36.583204Z",
     "iopub.status.idle": "2024-04-16T04:16:36.588852Z",
     "shell.execute_reply": "2024-04-16T04:16:36.587786Z"
    },
    "papermill": {
     "duration": 0.042049,
     "end_time": "2024-04-16T04:16:36.591179",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.549130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "adapter_path = '/kaggle/input/phi2-public-data-sft-adapter/pytorch/public-data-sft/1/phi2_public_data_sft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60dfce09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.658845Z",
     "iopub.status.busy": "2024-04-16T04:16:36.658003Z",
     "iopub.status.idle": "2024-04-16T04:16:36.674934Z",
     "shell.execute_reply": "2024-04-16T04:16:36.673955Z"
    },
    "papermill": {
     "duration": 0.0534,
     "end_time": "2024-04-16T04:16:36.677040",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.623640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhiModelRecover:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'phi'\n",
    "        self.load_model()\n",
    "        self.input_token_len = 1024\n",
    "        self.output_token_len = 100 \n",
    "        \n",
    "    # Load tokenizer and model\n",
    "    def load_model(self):\n",
    "        model_path = CFG.model_paths[self.model_name]\n",
    "        print(f\"model_path = {model_path}\")\n",
    "         # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        # Load the pretrained LLM in 4bit quantization  \n",
    "        q_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        # Load the model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          trust_remote_code=True,\n",
    "                                                          quantization_config=q_config)\n",
    "        # Load PEFT adapter to the model\n",
    "        self.model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        print(f\"Complete loading PEFT adapter {adapter_path}\")\n",
    "        self.model.to(CFG.DEVICE)\n",
    "        self.model.eval()\n",
    "        print(\"Complete loading the model\")\n",
    "        \n",
    "    # Generate the prompts using Phi models\n",
    "    def prompt_generate(self, original_text, rewrite_text):\n",
    "        prompt = f\"\"\"Instruct: Original Text:{original_text}\\n\n",
    "                     Rewritten Text:{rewrite_text}\\n\n",
    "                     Write a prompt that was likely given to the LLM to rewrite original text\n",
    "                     to rewritten text.\\nOutput:\"\"\"\n",
    "        # print(f\"prompt = {prompt}\")\n",
    "        # Tokenize the prompt and truncate to '1024' tokens\n",
    "        inputs = self.tokenizer(prompt, max_length=self.input_token_len,\n",
    "                                truncation=True, return_tensors=\"pt\", return_attention_mask=False)\n",
    "        try:\n",
    "            max_length = len(inputs.input_ids[0]) + self.output_token_len\n",
    "            #print(f\"max_length = {max_length}\")\n",
    "            # Move inputs to GPU\n",
    "            inputs = {k:v.to(CFG.DEVICE) for k,v in inputs.items()}\n",
    "            # print(f\"inputs = {inputs}\")        \n",
    "            # Generate the prompt\n",
    "            outputs = self.model.generate(**inputs,\n",
    "                                         do_sample=False,\n",
    "                                         max_length=max_length,\n",
    "                                         pad_token_id=self.tokenizer.pad_token_id)\n",
    "            # Encode the output to texts (strings)\n",
    "            text = self.tokenizer.batch_decode(outputs,\n",
    "                                               skip_special_tokens=True,\n",
    "                                               clean_up_tokenization_spaces=False)[0]\n",
    "            text_arr = text.split(\"Output:\")\n",
    "            generated_prompt = text_arr[1].strip()\n",
    "            # print(f\"generated_prompt = {generated_prompt}\")\n",
    "            return generated_prompt\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: {e}\")\n",
    "            pass # Add the default prompt if errors occur\n",
    "    \n",
    "    def infer(self, test_df):\n",
    "        default_prompt = \"\"\"Please improve the following text using the writing style of, \n",
    "                            maintaining the original meaning but altering the tone, diction, \n",
    "                            and stylistic elements to match the new style.Enhance the clarity, \n",
    "                            elegance, and impact of the following text by adopting the writing style of,\n",
    "                            ensuring the core message remains intact while transforming the tone,\n",
    "                            word choice, and stylistic features to align with the specified style.\"\"\"\n",
    "        rewrite_prompts = []\n",
    "        for i in range(len(test_df)):\n",
    "            row = test_df.iloc[i]\n",
    "            prompt = default_prompt\n",
    "            try:\n",
    "                prompt = self.prompt_generate(row['original_text'], row['rewritten_text'])\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\")\n",
    "                pass # Add the default prompt if errors occur\n",
    "            rewrite_prompts.append(prompt)\n",
    "        del self.model, self.tokenizer\n",
    "        return rewrite_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03cc0746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.744276Z",
     "iopub.status.busy": "2024-04-16T04:16:36.743014Z",
     "iopub.status.idle": "2024-04-16T04:16:36.750150Z",
     "shell.execute_reply": "2024-04-16T04:16:36.749135Z"
    },
    "papermill": {
     "duration": 0.043216,
     "end_time": "2024-04-16T04:16:36.752290",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.709074",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBMISSION = False\n",
    "if SUBMISSION:\n",
    "    recover = PhiModelRecover() \n",
    "    rewrite_prompts = recover.infer(test_df)\n",
    "    print(f\"rewrite_prompts = {rewrite_prompts}\")\n",
    "    del recover\n",
    "    # Submission\n",
    "    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n",
    "    submission[\"rewrite_prompt\"] = rewrite_prompts\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    display(submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a671a39",
   "metadata": {
    "papermill": {
     "duration": 0.032595,
     "end_time": "2024-04-16T04:16:36.818041",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.785446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate the prompts using pretrained Gemma-7b LLM <a class='anchor' id='llm'></a>\n",
    "Use pretrained Gemma-7b LLM to generate the prompts directly from testing data.\n",
    "- @RENOIR [Perplexity Baseline [Phi-2,Gemma-7b-it]](https://www.kaggle.com/code/itahiro/perplexity-baseline-phi-2-gemma-7b-it)\n",
    "- @PSI [h2oGPT Perplexity Ranking](https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca87020d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.884366Z",
     "iopub.status.busy": "2024-04-16T04:16:36.883533Z",
     "iopub.status.idle": "2024-04-16T04:16:36.888618Z",
     "shell.execute_reply": "2024-04-16T04:16:36.887613Z"
    },
    "papermill": {
     "duration": 0.040961,
     "end_time": "2024-04-16T04:16:36.890898",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.849937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef33161e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:36.959319Z",
     "iopub.status.busy": "2024-04-16T04:16:36.958602Z",
     "iopub.status.idle": "2024-04-16T04:16:36.966914Z",
     "shell.execute_reply": "2024-04-16T04:16:36.965960Z"
    },
    "papermill": {
     "duration": 0.044917,
     "end_time": "2024-04-16T04:16:36.969135",
     "exception": false,
     "start_time": "2024-04-16T04:16:36.924218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perplexity is a metric that measures the quality of language models\n",
    "# Perplexity is calculated as the exponent of the loss obtained from the model.\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        #perplexity = torch.exp(perplexity)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "661c9e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.034873Z",
     "iopub.status.busy": "2024-04-16T04:16:37.034160Z",
     "iopub.status.idle": "2024-04-16T04:16:37.039979Z",
     "shell.execute_reply": "2024-04-16T04:16:37.038958Z"
    },
    "papermill": {
     "duration": 0.041149,
     "end_time": "2024-04-16T04:16:37.042127",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.000978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewrite_prompt_templates = [\n",
    "\"\"\"Please improve this text using the writing style with maintaining the original meaning but\n",
    "   altering the tone.\"\"\",\n",
    "\"\"\"Please improve the following text by reimagining it through the lens of \n",
    "   [insert desired style here], retaining the original essence while elevating its clarity,\n",
    "   eloquence, and potency by modulating the tone, word choice, and stylistic nuances to \n",
    "   harmoniously embody the stylistic features while ensuring the core message remains intact.\"\"\",\n",
    "\"\"\"Please improve the following text using the writing style of, \n",
    "   maintaining the original meaning but altering the tone, diction,  \n",
    "   and stylistic elements to match the new style.Enhance the clarity, \n",
    "   elegance, and impact of the following text by adopting the writing style of,\n",
    "   ensuring the core message remains intact while transforming the tone,\n",
    "   word choice, and stylistic features to align with the specified style.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4254643d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.108657Z",
     "iopub.status.busy": "2024-04-16T04:16:37.107748Z",
     "iopub.status.idle": "2024-04-16T04:16:37.124672Z",
     "shell.execute_reply": "2024-04-16T04:16:37.123807Z"
    },
    "papermill": {
     "duration": 0.052231,
     "end_time": "2024-04-16T04:16:37.126760",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.074529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GemmaModelRecover:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'gemma-7b'\n",
    "        self.perp_nn = Perplexity() # Compute the perplexity\n",
    "        self.load_model()\n",
    "        \n",
    "    # Load tokenizer and model\n",
    "    def load_model(self):\n",
    "        model_path = CFG.model_paths[self.model_name]\n",
    "        print(f\"model_path = {model_path}\")\n",
    "         # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        # Load the pretrained LLM in 4bit quantization  \n",
    "        q_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        # Load the model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          trust_remote_code=True,\n",
    "                                                          quantization_config=q_config)\n",
    "        print(\"Complete loading the model\")\n",
    "        \n",
    "    # Infer the prompt for given texts (df)\n",
    "    def infer(self, df):\n",
    "        prompts = []\n",
    "        for idx in range(len(df)):\n",
    "            row = df.iloc[idx]\n",
    "            p_scores = []\n",
    "            with torch.no_grad():\n",
    "                 # # Combine the rewrite prompt with row data (original text, rewritten text) as a prompt\n",
    "                rw_prompts = []\n",
    "                for rw_prompt in rewrite_prompt_templates:\n",
    "                    rw_prompt = rw_prompt.replace(\"\\n\", \" \")\n",
    "                    rw_prompt = rw_prompt.replace(\"\\s+\", \" \")\n",
    "                    # print(f\"rw_prompt = {rw_prompt}\")\n",
    "                    rw_prompts.append(f\"\"\"<start_of_turn>\n",
    "                                            user {rw_prompt} {row[\"original_text\"]}\n",
    "                                          <end_of_turn>\n",
    "                                          <start_of_turn>\n",
    "                                              model{row[\"rewritten_text\"]}\n",
    "                                          <end_of_turn>\"\"\")\n",
    "                # Encode prompts to embeddings\n",
    "                inputs = self.tokenizer(rw_prompts, return_tensors=\"pt\",\n",
    "                                        add_special_tokens=False,\n",
    "                                        padding=True, truncation=True).to(CFG.DEVICE)\n",
    "                # Get the output\n",
    "                output = self.model(input_ids=inputs[\"input_ids\"],\n",
    "                                    attention_mask=inputs[\"attention_mask\"])\n",
    "                logits = output.logits\n",
    "\n",
    "                labels = inputs[\"input_ids\"]\n",
    "                # Attention masks has three kinds of scores:\n",
    "                # 1 = attend; 0 = ignore; -100: nullifying their impact on the sequence.\n",
    "                labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100) # -100 \n",
    "\n",
    "                # Compute the perplexity of model output (logits) and actual labels\n",
    "                for i in range(len(rewrite_prompt_templates)):\n",
    "                    p_score = self.perp_nn(logits[i].unsqueeze(0), \n",
    "                                           labels[i].unsqueeze(0))\n",
    "                    p_scores.append(p_score.detach().cpu())\n",
    "                del inputs, labels, output, logits\n",
    "            # Convert 'perps' as numpy array\n",
    "            p_scores = np.array(p_scores)\n",
    "            # Display the perplexity metric\n",
    "            print(f\"p_scores = {p_scores}\")\n",
    "            # Get the best output results of the lowest \n",
    "            best_pred = [np.array(rewrite_prompt_templates)[np.argsort(p_scores)][0]]\n",
    "            print(f\"best_pred = {best_pred}\")\n",
    "            prompts.append(str(best_pred[0]))\n",
    "            clear_memory()\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3161ba6d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.192248Z",
     "iopub.status.busy": "2024-04-16T04:16:37.191663Z",
     "iopub.status.idle": "2024-04-16T04:16:37.197278Z",
     "shell.execute_reply": "2024-04-16T04:16:37.196321Z"
    },
    "papermill": {
     "duration": 0.040932,
     "end_time": "2024-04-16T04:16:37.199670",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.158738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBMISSION = False\n",
    "if SUBMISSION:\n",
    "    recover = GemmaModelRecover() \n",
    "    rewrite_prompts = recover.infer(test_df)\n",
    "    print(f\"rewrite_prompts = {rewrite_prompts}\")\n",
    "    del recover\n",
    "    # Submission\n",
    "    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n",
    "    submission[\"rewrite_prompt\"] = rewrite_prompts\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    display(submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab5a05",
   "metadata": {
    "papermill": {
     "duration": 0.031199,
     "end_time": "2024-04-16T04:16:37.262217",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.231018",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate the prompts using pretrained Mistral-8x7b/ Mistral-7b-v2 <a class='anchor' id='mistral'></a>\n",
    "Use pretrained Mistral LLM to generate the prompts directly from testing data. No training is required. To guide its predictions, provide a few examples and leverage the Mistral LLM to predict prompts for the testing data.\n",
    "\n",
    "- @RICH OLSON [Mistral 7B Prompt Recovery (Version 2)](https://www.kaggle.com/code/richolson/mistral-7b-prompt-recovery-version-2)\n",
    "- @AATIF FRAZ [Prompt Prediction w/ Mixtral/Mistral7B/Gemma/Llama](https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6411e1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.327787Z",
     "iopub.status.busy": "2024-04-16T04:16:37.327425Z",
     "iopub.status.idle": "2024-04-16T04:16:37.332081Z",
     "shell.execute_reply": "2024-04-16T04:16:37.331178Z"
    },
    "papermill": {
     "duration": 0.039898,
     "end_time": "2024-04-16T04:16:37.334139",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.294241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fd8de54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.398015Z",
     "iopub.status.busy": "2024-04-16T04:16:37.397707Z",
     "iopub.status.idle": "2024-04-16T04:16:37.402195Z",
     "shell.execute_reply": "2024-04-16T04:16:37.401284Z"
    },
    "papermill": {
     "duration": 0.038771,
     "end_time": "2024-04-16T04:16:37.404209",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.365438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Disable effiency to avoid the issues reported by https://github.com/Lightning-AI/lit-gpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "914126c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.466362Z",
     "iopub.status.busy": "2024-04-16T04:16:37.465760Z",
     "iopub.status.idle": "2024-04-16T04:16:37.471268Z",
     "shell.execute_reply": "2024-04-16T04:16:37.470280Z"
    },
    "papermill": {
     "duration": 0.03838,
     "end_time": "2024-04-16T04:16:37.473220",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.434840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MISTRAL_INSTRUCTION = \"\"\"\n",
    "Provide the new text and I will tell you what new element was added or change in tone was made\n",
    "to improve it - with no references to the original.\n",
    "I will avoid mentioning names of characters.\n",
    "It is crucial no person, place or thing from the original text be mentioned.\n",
    "For example - I will not say things like 'change the puppet show into a book report'\n",
    "- I would just say 'improve this text into a book report'.\n",
    "If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.\n",
    "For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.\n",
    "My answer will be a single sentence.\"\"\"\n",
    "\n",
    "MISTRAL_INSTRUCTION = MISTRAL_INSTRUCTION.replace(\"\\n\", \" \")\n",
    "MISTRAL_INSTRUCTION = MISTRAL_INSTRUCTION.replace(\"\\s+\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fdfbf5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:37.537101Z",
     "iopub.status.busy": "2024-04-16T04:16:37.536532Z",
     "iopub.status.idle": "2024-04-16T04:16:37.997411Z",
     "shell.execute_reply": "2024-04-16T04:16:37.996392Z"
    },
    "papermill": {
     "duration": 0.496027,
     "end_time": "2024-04-16T04:16:37.999681",
     "exception": false,
     "start_time": "2024-04-16T04:16:37.503654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>NaN</td>\n",
       "      <td>`` Jacques, you're digging your teeth into my ...</td>\n",
       "      <td>In the dim, smoky haze of the Parisian catacom...</td>\n",
       "      <td>Rewrite the story as a noir with gangsters and...</td>\n",
       "      <td>opuqnkOAyP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>There is wind in her eyes, playing with her la...</td>\n",
       "      <td>There's wind in her eyes, like a playful breez...</td>\n",
       "      <td>Rewrite the story as a comedy where they have ...</td>\n",
       "      <td>IVxxhZAhPH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Sometimes, a girl up and gets herself pregnant...</td>\n",
       "      <td>However it happens, it happens often enough th...</td>\n",
       "      <td>Rewrite the story with more absurd humor and r...</td>\n",
       "      <td>xwYYIkBtms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm a simple guy, and I do a simple job. Get i...</td>\n",
       "      <td>The music cuts out, and the hallway erupts in ...</td>\n",
       "      <td>Rewrite the story as a chase scene from a Holl...</td>\n",
       "      <td>SRzHlChqxE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>NaN</td>\n",
       "      <td>He never did care for anything. Not the smile ...</td>\n",
       "      <td>He never did care for anything. Not the smile ...</td>\n",
       "      <td>Rewrite the essay with a darker twist</td>\n",
       "      <td>ZrcyKRomnC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Darius hits the wall with a loud thump, flung ...</td>\n",
       "      <td>Darius hits the wall with a loud thump, flung ...</td>\n",
       "      <td>Rewrite by merging this prompt with: \"In a fan...</td>\n",
       "      <td>GPxUDTLGvY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>`` Dave, have you seen this?'' There is no ans...</td>\n",
       "      <td>Alas, poor Dave, a victim of fate's cruel hand...</td>\n",
       "      <td>Rewrite this as a Shakespearean tragedy</td>\n",
       "      <td>ljSaWTXjFc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>`` You know, Pickering,'' said the man, `` fir...</td>\n",
       "      <td>In the twilight realm of dreams and illusion, ...</td>\n",
       "      <td>Rewrite your description to be surreal and dre...</td>\n",
       "      <td>gZFRWPfQCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Santa, \\n \\n Thanks for the BB gas gun. I...</td>\n",
       "      <td>In the quaint town of Snow Creek, where snowfl...</td>\n",
       "      <td>Rewrite the story as a heartwarming tale</td>\n",
       "      <td>SCwCSnMXwE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Oh, thank you doctor. I've been excited to lea...</td>\n",
       "      <td>Oh, thank you doctor. I've been excited to lea...</td>\n",
       "      <td>Rewrite the essay so the results come back fro...</td>\n",
       "      <td>VwwAXvxbdL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                      original_text  \\\n",
       "2173         NaN  `` Jacques, you're digging your teeth into my ...   \n",
       "2159         NaN  There is wind in her eyes, playing with her la...   \n",
       "1370         NaN  Sometimes, a girl up and gets herself pregnant...   \n",
       "1522         NaN  I'm a simple guy, and I do a simple job. Get i...   \n",
       "1117         NaN  He never did care for anything. Not the smile ...   \n",
       "1754         NaN  Darius hits the wall with a loud thump, flung ...   \n",
       "2395         NaN  `` Dave, have you seen this?'' There is no ans...   \n",
       "1019         NaN  `` You know, Pickering,'' said the man, `` fir...   \n",
       "921          NaN  Dear Santa, \\n \\n Thanks for the BB gas gun. I...   \n",
       "202          NaN  Oh, thank you doctor. I've been excited to lea...   \n",
       "\n",
       "                                         rewritten_text  \\\n",
       "2173  In the dim, smoky haze of the Parisian catacom...   \n",
       "2159  There's wind in her eyes, like a playful breez...   \n",
       "1370  However it happens, it happens often enough th...   \n",
       "1522  The music cuts out, and the hallway erupts in ...   \n",
       "1117  He never did care for anything. Not the smile ...   \n",
       "1754  Darius hits the wall with a loud thump, flung ...   \n",
       "2395  Alas, poor Dave, a victim of fate's cruel hand...   \n",
       "1019  In the twilight realm of dreams and illusion, ...   \n",
       "921   In the quaint town of Snow Creek, where snowfl...   \n",
       "202   Oh, thank you doctor. I've been excited to lea...   \n",
       "\n",
       "                                         rewrite_prompt          id  \n",
       "2173  Rewrite the story as a noir with gangsters and...  opuqnkOAyP  \n",
       "2159  Rewrite the story as a comedy where they have ...  IVxxhZAhPH  \n",
       "1370  Rewrite the story with more absurd humor and r...  xwYYIkBtms  \n",
       "1522  Rewrite the story as a chase scene from a Holl...  SRzHlChqxE  \n",
       "1117              Rewrite the essay with a darker twist  ZrcyKRomnC  \n",
       "1754  Rewrite by merging this prompt with: \"In a fan...  GPxUDTLGvY  \n",
       "2395            Rewrite this as a Shakespearean tragedy  ljSaWTXjFc  \n",
       "1019  Rewrite your description to be surreal and dre...  gZFRWPfQCP  \n",
       "921            Rewrite the story as a heartwarming tale  SCwCSnMXwE  \n",
       "202   Rewrite the essay so the results come back fro...  VwwAXvxbdL  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10 examples of rewritten prompts \n",
    "df1 = pd.read_csv('/kaggle/input/rewrite-prompts-examples/rewrite_examples.csv')\n",
    "df2 = pd.read_csv('/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv')\n",
    "\n",
    "example_df = pd.concat([df1, df2], axis=0)\n",
    "example_df = example_df.sample(n=10) # Sample 10 examples\n",
    "del df1, df2\n",
    "\n",
    "display(example_df)\n",
    "examples = []\n",
    "# Add 10 examples\n",
    "for example_text, example_rewrite, example_prompt in zip(example_df['original_text'],\n",
    "                                                         example_df['rewritten_text'],\n",
    "                                                         example_df['rewrite_prompt']):\n",
    "    examples.append({\"role\": \"user\", \"content\": f\"Original Text: {example_text}\"})\n",
    "    examples.append({\"role\": \"assistant\", \"content\": MISTRAL_INSTRUCTION})\n",
    "    examples.append({\"role\": \"user\", \"content\": f\"Re-written Text: {example_rewrite}\"})\n",
    "    examples.append({\"role\": \"assistant\", \"content\": f\"The request was:  {example_prompt}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd4ada",
   "metadata": {
    "papermill": {
     "duration": 0.030937,
     "end_time": "2024-04-16T04:16:38.064740",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.033803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility function\n",
    "Utility function processes the output/responses from the Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7e19708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.128520Z",
     "iopub.status.busy": "2024-04-16T04:16:38.128131Z",
     "iopub.status.idle": "2024-04-16T04:16:38.141826Z",
     "shell.execute_reply": "2024-04-16T04:16:38.140918Z"
    },
    "papermill": {
     "duration": 0.047561,
     "end_time": "2024-04-16T04:16:38.143919",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.096358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mistral v02 tends to respond with the input after providing the answer  \n",
    "#This trims response text to the requested number of sentences (or first LF or double-space sequence)\n",
    "def trim_to_first_num_sentences(text, num_sentences):\n",
    "    if num_sentences <= 0:\n",
    "        return \"\" # Return empty string\n",
    "\n",
    "    # Split text at the first linefeed\n",
    "    text_chunks = text.split('\\n', 1)\n",
    "    first_chunk = text_chunks[0]\n",
    "\n",
    "    # Split the first chunk into sentences, considering the space after each period\n",
    "    sentences = [sentence.strip() for sentence in first_chunk.split('.') if sentence]\n",
    "\n",
    "    # If there's a linefeed, return the text up to the first linefeed\n",
    "    if len(text_chunks) > 1:\n",
    "        # Check if the first chunk has fewer sentences than x, and if so, just return it\n",
    "        if len(sentences) < num_sentences:\n",
    "            trimmed_text = first_chunk\n",
    "        else:\n",
    "            # Otherwise, trim to x sentences within the first chunk\n",
    "            trimmed_text = '. '.join(sentences[:num_sentences]).strip()\n",
    "    else:\n",
    "        # If there's no linefeed, determine if the number of sentences is less than or equal to x\n",
    "        if len(sentences) <= num_sentences:\n",
    "            trimmed_text = '. '.join(sentences).strip()  # Ensure space is preserved after periods\n",
    "        else:\n",
    "            # Otherwise, return the first x sentences, again ensuring space after periods\n",
    "            trimmed_text = '. '.join(sentences[:num_sentences]).strip()\n",
    "\n",
    "    # Add back the final period if it was removed and the text needs to end with a sentence.\n",
    "    if len(sentences) > 0 and not trimmed_text.endswith('.'):\n",
    "        trimmed_text += '.'\n",
    "\n",
    "    return trimmed_text\n",
    "\n",
    "\n",
    "# Get text after last [/INST]\n",
    "def trim_output(text):\n",
    "    TERMINATE = \"[/INST]\"\n",
    "    text = text.replace('</s>', '')\n",
    "    #just in case it puts things in quotes\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    # Get the last [/INST]\n",
    "    last_pos = text.rfind(TERMINATE)\n",
    "    return text[last_pos + len(TERMINATE):] if last_pos != -1 else text\n",
    "\n",
    "# remove all number bullets\n",
    "def remove_numbered_bullets(text):\n",
    "    processed_lines = []\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        # Split each line at the first occurrence of '. '\n",
    "        parts = line.split('. ', 1)\n",
    "        # Part is likely a numbered list item, remove the numbering\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            processed_lines.append(parts[1])\n",
    "        else: # Not a numbered lis. Add the line\n",
    "            processed_lines.append(line)\n",
    "    # Combine all processed lines to a single text\n",
    "    return '\\n'.join(processed_lines)\n",
    "\n",
    "# Returns only response text that occurs after \"the request was: \"\n",
    "# for example, \"The request was:  Improve this text by making it a shanty.\"\n",
    "def get_response(text):\n",
    "    repsonse = text\n",
    "    parts = text.rsplit(\"The request was: \", 1)\n",
    "    if len(parts) > 1: # Check if the text contain \"The request was: \"\n",
    "        response = parts[1].strip()  # Get the texts after \"The request was\"\n",
    "    #Clean up numbered lists\n",
    "    response = remove_numbered_bullets(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a58a12",
   "metadata": {
    "papermill": {
     "duration": 0.031326,
     "end_time": "2024-04-16T04:16:38.206191",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.174865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b29d369",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.270868Z",
     "iopub.status.busy": "2024-04-16T04:16:38.270073Z",
     "iopub.status.idle": "2024-04-16T04:16:38.275300Z",
     "shell.execute_reply": "2024-04-16T04:16:38.274347Z"
    },
    "papermill": {
     "duration": 0.040733,
     "end_time": "2024-04-16T04:16:38.277674",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.236941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt = \"\"\"\n",
    "Refine the following passage by emulating the writing style of [insert desired style here], \n",
    "with a focus on enhancing its clarity, elegance, and overall impact.\n",
    "Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.\n",
    "Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.\n",
    "Enhance the clarity, elegance, and impact of the following text by adopting the writing style of ,\n",
    "ensuring the core message remains intact while transforming the tone, word choice, and stylistic features\n",
    "to align with the specified style.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddcc6f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.344167Z",
     "iopub.status.busy": "2024-04-16T04:16:38.343784Z",
     "iopub.status.idle": "2024-04-16T04:16:38.359338Z",
     "shell.execute_reply": "2024-04-16T04:16:38.358354Z"
    },
    "papermill": {
     "duration": 0.050611,
     "end_time": "2024-04-16T04:16:38.361564",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.310953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MistralModelRecover:\n",
    "    def __init__(self, examples=examples):\n",
    "        self.model_name = 'mistral-7b-v2'\n",
    "        self.examples = examples\n",
    "        self.max_new_tokens = 40 # number of generated prompts (output)\n",
    "        self.max_sentences = 1 # number of sentences of generated prompts (output)\n",
    "        self.load_model()\n",
    "        \n",
    "    # Load tokenizer and model\n",
    "    def load_model(self):\n",
    "        model_path = CFG.model_paths[self.model_name]\n",
    "        print(f\"model_path = {model_path}\")\n",
    "         # Load the tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        self.tokenizer.add_eos_token = True\n",
    "        self.tokenizer.add_bos_token = True\n",
    "        # Load the pretrained LLM in 4bit quantization  \n",
    "        q_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        # Get model configuration\n",
    "        config = AutoConfig.from_pretrained(model_path)\n",
    "        config.gradient_checkpointing = True\n",
    "        # Load the model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                          device_map=\"auto\",\n",
    "                                                          trust_remote_code=True,\n",
    "                                                          quantization_config=q_config, \n",
    "                                                          torch_dtype=torch.bfloat16,\n",
    "                                                          config=config) \n",
    "        print(f\"Complete loading the model\")\n",
    "      \n",
    "    def generate_prompt(self, original_text, rewritten_text):\n",
    "        messages = self.examples.copy() \n",
    "        # Add testing data\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Original Text: {original_text}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": MISTRAL_INSTRUCTION})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Re-written Text: {rewritten_text}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"The request was:  Improve this text by\"})\n",
    "\n",
    "        # Pass messages to Mistral\n",
    "        model_inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        # Move to GPUs\n",
    "        model_inputs = model_inputs.to(CFG.DEVICE) \n",
    "        # Generate the prompts \n",
    "        generated_ids = self.model.generate(model_inputs,\n",
    "                                            max_new_tokens=self.max_new_tokens,\n",
    "                                            pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode and trim to actual response\n",
    "        decoded_output = self.tokenizer.batch_decode(generated_ids)\n",
    "        # print(f\"decoded_output[0] = {decoded_output[0]}\")\n",
    "        trimed_output = trim_output(decoded_output[0])\n",
    "        # print(f\"trimed_output = {trimed_output}\")\n",
    "        response = get_response(trimed_output)\n",
    "        # Trim the first number of sentences\n",
    "        # print(f\"Before trimming first number of sentences: {response}\")\n",
    "        response = trim_to_first_num_sentences(response, self.max_sentences)\n",
    "        # print(f\"After trimming first number of sentences: {response}\")\n",
    "        del messages, model_inputs, generated_ids, decoded_output, trimed_output\n",
    "        clear_memory()\n",
    "        return response\n",
    "\n",
    "    # Infer the prompt for given texts (df)\n",
    "    def infer(self, df):\n",
    "        prompts = []\n",
    "        for idx in range(len(df)):\n",
    "            row = df.iloc[idx]\n",
    "            prompt = self.generate_prompt(row['original_text'], row['rewritten_text'])\n",
    "            #default to baseline if empty or unusually short\n",
    "            if len(prompt) < 15:\n",
    "                prompt = default_prompt\n",
    "            prompts.append(prompt)\n",
    "            \n",
    "        del self.model, self.tokenizer\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67d5e787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.426960Z",
     "iopub.status.busy": "2024-04-16T04:16:38.426496Z",
     "iopub.status.idle": "2024-04-16T04:16:38.433376Z",
     "shell.execute_reply": "2024-04-16T04:16:38.432245Z"
    },
    "papermill": {
     "duration": 0.042601,
     "end_time": "2024-04-16T04:16:38.435599",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.392998",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBMISSION = False\n",
    "if SUBMISSION:\n",
    "    recover = MistralModelRecover() \n",
    "    rewrite_prompts = recover.infer(test_df)\n",
    "    print(f\"rewrite_prompts = {rewrite_prompts}\")\n",
    "    del recover\n",
    "    # Submission\n",
    "    submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n",
    "    submission[\"rewrite_prompt\"] = rewrite_prompts\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    display(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc047e",
   "metadata": {
    "papermill": {
     "duration": 0.033646,
     "end_time": "2024-04-16T04:16:38.502897",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.469251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modify the prompts using Spacy <a class='anchor' id='mean-prompt'></a>\n",
    "This approach is inspired by @ RICH OLSON [Spacy mean-prompt modification (just CPU!)](https://www.kaggle.com/code/richolson/spacy-mean-prompt-modification-just-cpu/notebook)\n",
    "\n",
    "- **Identify unique words:** find words present in the rewritten text but absent from the original.\n",
    "- **Discover similar words:** search for semantically similar words (synonyms) for this set of unique words.\n",
    "- **Replace unique words with similar words:** replace these unique words with their similar words while ensuring they maintain the same part of speech. \n",
    "\n",
    "The goal is to refine the prompt to bring it closer to the actual prompt, hopefully improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81e7b1",
   "metadata": {
    "papermill": {
     "duration": 0.03342,
     "end_time": "2024-04-16T04:16:38.570073",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.536653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcd78637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.636139Z",
     "iopub.status.busy": "2024-04-16T04:16:38.635340Z",
     "iopub.status.idle": "2024-04-16T04:16:38.640256Z",
     "shell.execute_reply": "2024-04-16T04:16:38.639263Z"
    },
    "papermill": {
     "duration": 0.039789,
     "end_time": "2024-04-16T04:16:38.642493",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.602704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "REWRITE_PREFIX = 'Please improve this text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "499e6ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:16:38.711577Z",
     "iopub.status.busy": "2024-04-16T04:16:38.710785Z",
     "iopub.status.idle": "2024-04-16T04:17:03.936895Z",
     "shell.execute_reply": "2024-04-16T04:17:03.935836Z"
    },
    "papermill": {
     "duration": 25.263677,
     "end_time": "2024-04-16T04:17:03.939660",
     "exception": false,
     "start_time": "2024-04-16T04:16:38.675983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import spacy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Load SpaCy's English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# loads spacy's vocuabulary...\n",
    "for s in nlp.vocab.vectors:\n",
    "    _ = nlp.vocab[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f60da94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:04.008756Z",
     "iopub.status.busy": "2024-04-16T04:17:04.007208Z",
     "iopub.status.idle": "2024-04-16T04:17:04.013945Z",
     "shell.execute_reply": "2024-04-16T04:17:04.012931Z"
    },
    "papermill": {
     "duration": 0.042988,
     "end_time": "2024-04-16T04:17:04.016766",
     "exception": false,
     "start_time": "2024-04-16T04:17:03.973778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#how many words get swapped out of base prompt\n",
    "words_to_swap = 5\n",
    "\n",
    "#start this many away from the word that most represent the entire text \n",
    "#increasing this number will result in more specific words (0 = most general)\n",
    "word_semantic_offset = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04da1de",
   "metadata": {
    "papermill": {
     "duration": 0.03361,
     "end_time": "2024-04-16T04:17:04.088438",
     "exception": false,
     "start_time": "2024-04-16T04:17:04.054828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1. Identify unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ada2c402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:04.152430Z",
     "iopub.status.busy": "2024-04-16T04:17:04.152057Z",
     "iopub.status.idle": "2024-04-16T04:17:04.160752Z",
     "shell.execute_reply": "2024-04-16T04:17:04.159667Z"
    },
    "papermill": {
     "duration": 0.043009,
     "end_time": "2024-04-16T04:17:04.163119",
     "exception": false,
     "start_time": "2024-04-16T04:17:04.120110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the texts that contain the words unique to rewrites (not in originals)\n",
    "def identify_unique_words(original_text, rewrite_text):\n",
    "    # Tokenize original and rewrite text using NLP\n",
    "    original_doc = nlp(original_text)\n",
    "    rewrite_doc = nlp(rewrite_text)\n",
    "    # Collect all nouns and verbs from original text as a set of removing words\n",
    "    words_to_remove = set([token.lemma_.lower() for token in original_doc\n",
    "                           if token.pos_ in [\"NOUN\", \"VERB\"] or token.is_stop])\n",
    "    \n",
    "    # Remove nouns and verbs from the rewrite text, ensuring no noun and verb duplicates\n",
    "    words_to_keep = OrderedDict((token.text, None) for token in rewrite_doc \n",
    "                             if token.lemma_.lower() not in words_to_remove \n",
    "                             and token.pos_ != \"PROPN\" and token.text_with_ws.strip())\n",
    "    \n",
    "    # Combine all kept words to form the filtered text\n",
    "    filtered_text = \" \".join(words_to_keep.keys())\n",
    "    filtered_text = filtered_text.replace(\"'s\", \"\") # remove 's\n",
    "    filtered_text = filtered_text.replace('\"', '') # remove \"\n",
    "    # print(f\"filtered_text = {filtered_text}\")\n",
    "    unique_words = nlp(filtered_text) # Tokenize the text to doc\n",
    "#     print(f\"Unique words = {unique_words}\")\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87d41d",
   "metadata": {
    "papermill": {
     "duration": 0.030858,
     "end_time": "2024-04-16T04:17:04.226383",
     "exception": false,
     "start_time": "2024-04-16T04:17:04.195525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 2. Discover similar words of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79584afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:04.290767Z",
     "iopub.status.busy": "2024-04-16T04:17:04.290372Z",
     "iopub.status.idle": "2024-04-16T04:17:06.889072Z",
     "shell.execute_reply": "2024-04-16T04:17:06.888168Z"
    },
    "papermill": {
     "duration": 2.634318,
     "end_time": "2024-04-16T04:17:06.891579",
     "exception": false,
     "start_time": "2024-04-16T04:17:04.257261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect all words from NLP library\n",
    "all_words = [word for word in nlp.vocab \n",
    "             if word.has_vector and not word.is_stop # filter stop words, alpahbets, and words of single character (like a)\n",
    "             and word.is_alpha and len(word.text) > 1]\n",
    "\n",
    "def get_most_similar_words(unique_words):\n",
    "#     print(f\"unique_words = {unique_words}\")\n",
    "    # Get word vectors of all unique words\n",
    "    unique_word_vectors = [token.vector for token in unique_words if token.has_vector]\n",
    "    if unique_word_vectors:\n",
    "        # Get the mean vector\n",
    "        average_word_vector = np.mean(unique_word_vectors, axis=0) # Compute the average word vector\n",
    "        # print(f\"all_words = {all_words}\")\n",
    "#         print(f\"all_words[0] = {all_words[0]}\")\n",
    "        # Get word vectors to compute word similarity\n",
    "        all_word_vectors = np.array([word.vector for word in all_words])\n",
    "        # Compute cosine distances and find the closest words\n",
    "        distances = distance.cdist([average_word_vector], all_word_vectors, \"cosine\")[0]\n",
    "        # Sort the distance (from)\n",
    "        sorted_indexes = distances.argsort()[:(word_semantic_offset + words_to_swap)]\n",
    "        # Sort the words by its similarity to average vector\n",
    "        sorted_words = [all_words[idx].text for idx in sorted_indexes]\n",
    "        # print(f\"sorted words = {sorted_words}\")\n",
    "        similar_words = sorted_words[words_to_swap * -1:]\n",
    "#         print(f\"Most similar word to average unique word = {similar_words}\")\n",
    "        # Map the similar words to their parts of speech (pos) for a single use\n",
    "        similar_words_by_pos = {}\n",
    "        for token in nlp(\" \".join(similar_words)):\n",
    "            if token.pos_ not in similar_words_by_pos:\n",
    "                similar_words_by_pos[token.pos_] = []\n",
    "            similar_words_by_pos[token.pos_].append(token.text)\n",
    "        return similar_words, similar_words_by_pos\n",
    "    else:\n",
    "        print(\"No average vector could be computed.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6bd5d8",
   "metadata": {
    "papermill": {
     "duration": 0.033253,
     "end_time": "2024-04-16T04:17:06.958446",
     "exception": false,
     "start_time": "2024-04-16T04:17:06.925193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 3. Replace the prompt with simiar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa4309be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:07.025343Z",
     "iopub.status.busy": "2024-04-16T04:17:07.024900Z",
     "iopub.status.idle": "2024-04-16T04:17:07.036412Z",
     "shell.execute_reply": "2024-04-16T04:17:07.035383Z"
    },
    "papermill": {
     "duration": 0.04762,
     "end_time": "2024-04-16T04:17:07.038559",
     "exception": false,
     "start_time": "2024-04-16T04:17:06.990939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Substitutes words matching part-of-speech, managing spacing, preserves structure\n",
    "def replace_prompt_with_similar_words_pos(doc_prompt, similar_words, similar_words_by_pos):\n",
    "    rewrite_words = [] # \n",
    "    # List of POS tags of similar words \n",
    "    pos_tags = [token.pos_ for token in nlp(\" \".join(similar_words))]  \n",
    "    # Iterate over the doc tokens\n",
    "    for token in doc_prompt:\n",
    "        rewrite_word = token.text_with_ws\n",
    "        # Check if the token's POS is in our list of POS tags for substitution\n",
    "        if token.pos_ in pos_tags and not token.is_punct and not token.is_space:\n",
    "            for pos in pos_tags:\n",
    "                if token.pos_ == pos and pos in similar_words_by_pos and similar_words_by_pos[pos]:\n",
    "                    rewrite_word = similar_words_by_pos[pos].pop(0)\n",
    "                    break\n",
    "        rewrite_words.append(rewrite_word)\n",
    "    # Combine words to a text\n",
    "    return \" \".join(rewrite_words)\n",
    "\n",
    "#returns prompt with relevant words swapped\n",
    "def replace_prompt_with_similar_words(rewrite_prompt, similar_words, similar_words_by_pos):\n",
    "    # Process the template sentence\n",
    "    doc_prompt = nlp(rewrite_prompt)\n",
    "\n",
    "    # Perform substitutions while managing spacing and punctuation\n",
    "    rewritten_prompt = replace_prompt_with_similar_words_pos(doc_prompt,\n",
    "                                                             similar_words,\n",
    "                                                             similar_words_by_pos)\n",
    "    # Clean up the prompt\n",
    "    rewritten_prompt = rewritten_prompt.lower()\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"  \", \" \")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"  \", \" \")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\" ,\", \",\")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"[ \", \"[\")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\" ]\", \"]\")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\" .\", \".\")\n",
    "    # Special case for incorrect spelling word 'somethin'?\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"somethin \", \"something \")\n",
    "    rewritten_prompt = rewritten_prompt.replace(\"somethin, \", \"something, \")\n",
    "    return f\"{rewritten_prompt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc01e8",
   "metadata": {
    "papermill": {
     "duration": 0.033011,
     "end_time": "2024-04-16T04:17:07.105086",
     "exception": false,
     "start_time": "2024-04-16T04:17:07.072075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load demo data \n",
    "Load @DIPAM CHAKRABORTY [3000 Rewritten texts - Prompt recovery Challenge](https://www.kaggle.com/datasets/dipamc77/3000-rewritten-texts-prompt-recovery-challenge) that contain original text, rewrite text and rewrite prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "360dd479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:07.169165Z",
     "iopub.status.busy": "2024-04-16T04:17:07.168160Z",
     "iopub.status.idle": "2024-04-16T04:17:07.175290Z",
     "shell.execute_reply": "2024-04-16T04:17:07.174351Z"
    },
    "papermill": {
     "duration": 0.041483,
     "end_time": "2024-04-16T04:17:07.177441",
     "exception": false,
     "start_time": "2024-04-16T04:17:07.135958",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "if TESTING:\n",
    "    demo_df = pd.read_csv(\"/kaggle/input/3000-rewritten-texts-prompt-recovery-challenge/prompts_0_500_wiki_first_para_3000.csv\")\n",
    "    demo_texts = demo_df[:5]\n",
    "    # Test with demo data\n",
    "    for index, row in demo_texts.iterrows():\n",
    "        print(\"------------\")\n",
    "        print(\"Actual Rewrite Prompt:\", row['rewrite_prompt'])\n",
    "        unique_words = identify_unique_words(row['original_text'], row['rewritten_text'])\n",
    "        # print(\"\\nUnique words:\", unique_words)\n",
    "        similar_words, similar_words_by_pos = get_most_similar_words(unique_words)\n",
    "        # print(\"\\nMost similar words to unqiue words:\", similar_words)\n",
    "        rewrite_prompt = 'Please improve this text using the writing style with maintaining the original meaning but altering the tone.'\n",
    "        print (\"\\nPredicted prompt:\", replace_prompt_with_similar_words(rewrite_prompt, similar_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1eca1",
   "metadata": {
    "papermill": {
     "duration": 0.032783,
     "end_time": "2024-04-16T04:17:07.243837",
     "exception": false,
     "start_time": "2024-04-16T04:17:07.211054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load and submit testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80d48b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:17:07.310621Z",
     "iopub.status.busy": "2024-04-16T04:17:07.310269Z",
     "iopub.status.idle": "2024-04-16T04:20:55.588942Z",
     "shell.execute_reply": "2024-04-16T04:20:55.587645Z"
    },
    "papermill": {
     "duration": 228.314192,
     "end_time": "2024-04-16T04:20:55.591546",
     "exception": false,
     "start_time": "2024-04-16T04:17:07.277354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path = /kaggle/input/gemma/transformers/7b-it/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6c2c587de84808acc00a1d878f41e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading the model\n",
      "p_scores = [7.1796036 7.777508  7.0690274]\n",
      "best_pred = ['Please improve the following text using the writing style of, \\n   maintaining the original meaning but altering the tone, diction,  \\n   and stylistic elements to match the new style.Enhance the clarity, \\n   elegance, and impact of the following text by adopting the writing style of,\\n   ensuring the core message remains intact while transforming the tone,\\n   word choice, and stylistic features to align with the specified style.']\n",
      "rewrite_prompts = ['Please improve the following text using the writing style of, \\n   maintaining the original meaning but altering the tone, diction,  \\n   and stylistic elements to match the new style.Enhance the clarity, \\n   elegance, and impact of the following text by adopting the writing style of,\\n   ensuring the core message remains intact while transforming the tone,\\n   word choice, and stylistic features to align with the specified style.']\n",
      "unique_words = Here shanty : ( Verse 1 ) , spun so clever they outrun . find bright crack code and shine light Chorus Oh my dear we compete Two thousand challenge grand guess hand over hand.(Verse 2 original treasure lost secret Similar words = ['overlaying', 'interleaved', 'overlapping', 'interleaving', 'thermosetting']\n",
      "Rewrite prompt spacy = please overlaying the overlapping thermosetting interleaving the writing style of, \n",
      " maintaining the interleaved meaning but altering the tone, diction, \n",
      " and stylistic elements to match the new style. enhance the clarity, \n",
      " elegance, and impact of the following text by adopting the writing style of, \n",
      " ensuring the core message remains intact while transforming the tone, \n",
      " word choice, and stylistic features to align with the specified style.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    please overlaying the overlapping thermosettin...\n",
       "Name: rewrite_prompt, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SUBMISSION = True\n",
    "if SUBMISSION:\n",
    "    test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "    test_df['original_text'] = test_df['original_text'].fillna('')\n",
    "    test_df['rewritten_text'] = test_df['rewritten_text'].fillna('')\n",
    "    # # Add a list of rewrite prompt\n",
    "#     rewrite_prompt = 'Please improve this text using the writing style with maintaining the original meaning but altering the tone.'\n",
    "#     rewrite_prompts = [rewrite_prompt for i in range(len(test_df))]\n",
    "    # # # # Use fine-tuned Phi to generate the prompts\n",
    "#     recover = PhiModelRecover() \n",
    "#     rewrite_prompts = recover.infer(test_df)\n",
    "    # # # Use pretrained Gemma-7b to generate the prompts\n",
    "    recover = GemmaModelRecover()  \n",
    "    rewrite_prompts = recover.infer(test_df)    \n",
    "    # Use Mistral-7b-v2 model to generate the prompts\n",
    "#     recover = MistralModelRecover() \n",
    "#     rewrite_prompts = recover.infer(test_df)\n",
    "    del recover\n",
    "    clear_memory()\n",
    "    print(f\"rewrite_prompts = {rewrite_prompts}\")\n",
    "    rewrite_prompts_spacy = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        unique_words = identify_unique_words(row['original_text'], row['rewritten_text'])\n",
    "        similar_words, similar_words_by_pos  = get_most_similar_words(unique_words)\n",
    "        print(f\"unique_words = {unique_words} Similar words = {similar_words}\")\n",
    "        rewrite_prompt = rewrite_prompts[index]\n",
    "        rewrite_prompt_spacy = replace_prompt_with_similar_words(rewrite_prompt,\n",
    "                                                                 similar_words,\n",
    "                                                                 similar_words_by_pos)\n",
    "        print(f\"Rewrite prompt spacy = {rewrite_prompt_spacy}\")\n",
    "        del unique_words, similar_words, similar_words_by_pos\n",
    "        rewrite_prompts_spacy.append(rewrite_prompt_spacy)\n",
    "    # Add rewrite prompts\n",
    "    test_df['rewrite_prompt'] = rewrite_prompts_spacy\n",
    "    test_df.to_csv('test_df.csv', index=False)\n",
    "    # Write to csv file\n",
    "    test_df[['id', 'rewrite_prompt']].to_csv('submission.csv', index=False)\n",
    "    display(test_df['rewrite_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044e27b",
   "metadata": {
    "papermill": {
     "duration": 0.034197,
     "end_time": "2024-04-16T04:20:55.663144",
     "exception": false,
     "start_time": "2024-04-16T04:20:55.628947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation metric <a class='anchor' id='evaluation'></a>\n",
    "Here, we describe how to evaluate the recovered prompts using cosine similarity with sentence-t5-base from HuggingFace:\n",
    "\n",
    "- **Compute Embeddings:**\n",
    "We first compute the sentence embeddings for both the predicted prompts and the original prompts using sentence-t5-base.\n",
    "- **Calculate Cosine Similarity:**\n",
    "For each pair of corresponding predicted and original prompts (each row), we calculate the cosine similarity. Cosine similarity measures how similar the embeddings are, ranging from -1 (completely dissimilar) to 1 (identical).\n",
    "\n",
    "- **Average the Scores:** \n",
    "Finally, we average the cosine similarity scores (or the cubed values if applied in step 3) across all prompt pairs. This provides a single overall score (CV score) representing the average similarity between predicted and original prompts.\n",
    "\n",
    "\n",
    "@SAMBHAV DIXIT [submission with evaluation metric](https://www.kaggle.com/code/sambhavdixit/submission-with-evaluation-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b712be4e",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-04-16T04:20:55.734006Z",
     "iopub.status.busy": "2024-04-16T04:20:55.733612Z",
     "iopub.status.idle": "2024-04-16T04:20:55.767129Z",
     "shell.execute_reply": "2024-04-16T04:20:55.766251Z"
    },
    "papermill": {
     "duration": 0.071983,
     "end_time": "2024-04-16T04:20:55.769704",
     "exception": false,
     "start_time": "2024-04-16T04:20:55.697721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def CVScore():\n",
    "    test = pd.read_csv(\"/kaggle/working/test_df.csv\")\n",
    "    \n",
    "    scs = lambda row: abs((cosine_similarity(row[\"actual_embeddings\"],\n",
    "                                             row[\"pred_embeddings\"])) ** 3)\n",
    "    \n",
    "    model = SentenceTransformer('/kaggle/input/sentence-t5-base-hf/sentence-t5-base')\n",
    "\n",
    "    test[\"actual_embeddings\"] = test[\"rewrite_prompt\"].apply(lambda x: model.encode(x, normalize_embeddings=True,\n",
    "                                                                                    show_progress_bar=False).reshape(1, -1))\n",
    "    test[\"pred_embeddings\"] = test[\"rewrite_prompt\"].apply(lambda x: model.encode(x, normalize_embeddings=True,\n",
    "                                                                                  show_progress_bar=False).reshape(1, -1))\n",
    "    test[\"score\"] = test.apply(scs, axis=1)\n",
    "    \n",
    "    return np.mean(test['score'])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e33537c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T04:20:55.842569Z",
     "iopub.status.busy": "2024-04-16T04:20:55.841588Z",
     "iopub.status.idle": "2024-04-16T04:20:55.846277Z",
     "shell.execute_reply": "2024-04-16T04:20:55.845284Z"
    },
    "papermill": {
     "duration": 0.042936,
     "end_time": "2024-04-16T04:20:55.848422",
     "exception": false,
     "start_time": "2024-04-16T04:20:55.805486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(f\"CV Score: {CVScore(test_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4516250,
     "sourceId": 7729275,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4518936,
     "sourceId": 7733314,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4506214,
     "sourceId": 7747717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4634330,
     "sourceId": 7893017,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4668661,
     "sourceId": 7940822,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4552503,
     "sourceId": 7994533,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4761052,
     "sourceId": 8069288,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 166559676,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169374254,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169375546,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169375710,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169375992,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169501845,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169507446,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169635716,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 169635789,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 171067769,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8658,
     "sourceId": 10716,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 5171,
     "sourceId": 11371,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11394,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 17852,
     "sourceId": 21555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 407.389337,
   "end_time": "2024-04-16T04:20:59.170289",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T04:14:11.780952",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "105ccb6426c6473c9d3e2e0e574dbf9f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "207102c66c2b4d2db312617836ed68fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_105ccb6426c6473c9d3e2e0e574dbf9f",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_81070157ca0a4ceea6b59646aa43754a",
       "value": 4.0
      }
     },
     "4baa7668430d47aea11123445e8287ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7ec15ab02bbd4df485b027189dde1a60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_867ab76ff68140ee931a587f11edbc8d",
       "placeholder": "â",
       "style": "IPY_MODEL_b1f50455be654977b61ffb8103c5eed1",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "81070157ca0a4ceea6b59646aa43754a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "867ab76ff68140ee931a587f11edbc8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a095bc8c3d1c454e84acf7cf8fe18503": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ece12104254b4037a6be7e2fe2513763",
       "placeholder": "â",
       "style": "IPY_MODEL_4baa7668430d47aea11123445e8287ae",
       "value": " 4/4 [03:33&lt;00:00, 49.01s/it]"
      }
     },
     "b1f50455be654977b61ffb8103c5eed1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b6f69648bbca49d2a37502e691f3a881": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db6c2c587de84808acc00a1d878f41e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7ec15ab02bbd4df485b027189dde1a60",
        "IPY_MODEL_207102c66c2b4d2db312617836ed68fd",
        "IPY_MODEL_a095bc8c3d1c454e84acf7cf8fe18503"
       ],
       "layout": "IPY_MODEL_b6f69648bbca49d2a37502e691f3a881"
      }
     },
     "ece12104254b4037a6be7e2fe2513763": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
